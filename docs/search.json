[
  {
    "objectID": "onboard-scaffold_services.html",
    "href": "onboard-scaffold_services.html",
    "title": "Working Together",
    "section": "",
    "text": "We are excited to work with your team and currently offer a range of options for what that collaboration might look like. The categories below are not exhaustive so if you think your needs will fall between or outside of these tasks, please don’t hesitate to contact our team to start that conversation!"
  },
  {
    "objectID": "onboard-scaffold_services.html#tasks",
    "href": "onboard-scaffold_services.html#tasks",
    "title": "Working Together",
    "section": "Tasks",
    "text": "Tasks\nThis level of collaboration is the core of our value to working groups! When your group identifies a data-related need (e.g., designing an analytical workflow, creating a website, writing an R Shiny app, etc.), you reach out to our team and get the conversation started. During that time we will work closely with you to define the scope of the work and get a clear picture of what “success” looks like in this context.\nOnce the task is appropriately defined, the conversation moves on to how independently you’d like us to work. This varies dramatically between tasks even within a single working group and there is no single right answer! For some tasks, we are capable of working completely independently and returning to your team with a finished product in hand for review but we are equally comfortable working closely with you throughout the life-cycle of a task."
  },
  {
    "objectID": "onboard-scaffold_services.html#analytical-sprints",
    "href": "onboard-scaffold_services.html#analytical-sprints",
    "title": "Working Together",
    "section": "Analytical Sprints",
    "text": "Analytical Sprints\nFor larger tasks, we offer “analytical sprints.” If your group requests an analytical sprint, you will get one of our team members working full time only for your group’s tasks for 3-4 weeks! This can be a great option if your group has many smaller tasks or fewer larger projects that are particularly time-sensitive.\nWe are excited that these sprints are a part of our “menu” of offerings to you all but please reach out to us to start the conversation before requesting a sprint so that we can make sure we are all on the same page."
  },
  {
    "objectID": "onboard-scaffold_services.html#weekly-office-hours",
    "href": "onboard-scaffold_services.html#weekly-office-hours",
    "title": "Working Together",
    "section": "Weekly Office Hours",
    "text": "Weekly Office Hours\nEach of our staff members offers a one-hour block weekly as a standing office hour each week. This is a great time to join us with small hurdles or obstacles you’re experiencing in a given week. For example, previous office hours have dealt with topics like refreshing on git/GitHub vocabulary, authenticating the googledrive R package, or solving a specific error in a new R script."
  },
  {
    "objectID": "onboard-scaffold_services.html#workshops-trainings",
    "href": "onboard-scaffold_services.html#workshops-trainings",
    "title": "Working Together",
    "section": "Workshops & Trainings",
    "text": "Workshops & Trainings\nIt is sometimes the case that your working group wants to become more familiar (or get a refresher) on a tool you’d like to include in your group’s workflow. To that end we can design and offer workshops on a selection of data science tools. Previous workshops include an introduction to GitHub, a primer on the tidyverse R packages, and a review of the fundamentals of R Shiny apps. Our past workshops are summarized here and we are happy to offer new workshops if your group wants a workshop on something within our knowledge base. These workshops are typically 1-2 hours but we can be flexible with that timing depending on your group’s needs.\nSimilarly, we also have been creating more ‘go at your own pace’-style tutorials that can be accessed here. These tutorials are usually smaller in scope than workshops but are still built to maximize value to your group either as review or first contact with a given subject. As with the workshops, we are happy to create new tutorials if something comes up for your team so please reach out and we can discuss further!"
  },
  {
    "objectID": "onboard-scaffold_infrastructure.html",
    "href": "onboard-scaffold_infrastructure.html",
    "title": "Computing Infrastructure",
    "section": "",
    "text": "NCEAS offers an array of helpful computing infrastructure for your group. These resources are purely optional but can be a significant aid to your projects’ progression both when you’re in-person in Santa Barbara and between those meetings! Our team is ready and willing to explain these in greater detail or help your group build them into your workflow so don’t hesitate to reach out!"
  },
  {
    "objectID": "onboard-scaffold_infrastructure.html#analytical-servers",
    "href": "onboard-scaffold_infrastructure.html#analytical-servers",
    "title": "Computing Infrastructure",
    "section": "Analytical Servers",
    "text": "Analytical Servers\nWorking groups have access to several high-performance computers at NCEAS, which provide advanced analytical, web, and database capabilities that far exceed the capabilities of desktop computers. We are available to set up and instruct you in the use of these Linux/Unix systems for demanding scientific analyses and modeling runs that benefit from lots of memory or storage, or access to multiple CPU’s.\n\nOur most powerful system (pictured) currently offers 384GB of RAM memory, along with 44 cores (CPUs), and several terabytes of fast storage. Several scientific software packages – such as R/RStudio, Python, Matlab, and QGIS – are already installed. Ask us if you need us to install any specific analytical libraries or packages.\nWe can also set up shared storage space on our server to facilitate data sharing within your working group. This is particularly valuable for large files that would take up a significant amount of your group’s cloud storage system (e.g., GoogleDrive, Dropbox, etc.)\nTo request access to the NCEAS analytical server, please coordinate with your PIs and contact us with your request."
  },
  {
    "objectID": "onboard-scaffold_infrastructure.html#data-preservation",
    "href": "onboard-scaffold_infrastructure.html#data-preservation",
    "title": "Computing Infrastructure",
    "section": "Data Preservation",
    "text": "Data Preservation\nWhile we know that you are just getting started, it is never too early to start thinking about where you are going to store the data your group will collate and synthesize! NCEAS is committed to practicing and promoting open science, making scientific research and its supporting data and information accessible to all levels of society. Therefore, we recommend that any input data used for your synthesis work be well documented and preserved in a long-term data repository.\n\nIt is required to document and preserve any products resulting from working group activities in a long-term data repository, such as the Environmental Data Initiative (EDI) data repository or the NCEAS-supported KNB Data Repository, which are part of the Data Observation Network for Earth (DataONE).\n\nWe are happy to discuss which of these options might fit your group best though we recognize this discussion can probably be left alone until your group has amassed a substantial amount of data."
  },
  {
    "objectID": "tutorial-scaffold_quarto-website.html",
    "href": "tutorial-scaffold_quarto-website.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "Quarto is a new tool developed by RStudio (the company, not the program) to create a more ‘what you see is what you get’ editor for creating markdown files and products (e.g., books, websites, etc.). Additionally, it includes a visual editor that allows users to insert headings and embed figures via buttons that are intuitively labeled rather than through somewhat arcane HTML text or symbols. While Quarto is still in its infancy, it is rapidly gathering a following due to the aforementioned visual editor and for the ease with which quarto documents and websites can be created.\n\nPrerequisites\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nDownload Quarto\nInstall git\nCreate a GitHub Account\nConnect git to GitHub\n\nFeel free to skip any steps that you have already completed!\n\n\nCreate a Quarto Website R Project\nTo begin, click the “Project” button in the top right of your RStudio session.\n\nIn the resulting dialogue, click the “New Directory” option.\n\n\n\nFrom the list of options for project templates, select “Quarto Website”.\n\n\n\nPick a title and check the “Create a git repository” checkbox. For your title, short but descriptive titles are most effective. Once that is done, click “Create Project” in the bottom right of the window.\n\n\n\nAfter a few seconds, RStudio should refresh with a Quarto document (such documents have the file extension “.qmd”) and a “_quarto.yml” file open.\n\n\n\nPart of Quarto’s central philosophy is that all of the formatting of individual .qmd files in a project is governed by the settings created by a singular .yml file. In an R markdown project some of the global settings are set in .yml but other settings are handled within each .Rmd file. This centralization is a key innovation in streamlining projects and is one reason for Quarto’s quick popularity.\n\n\nPreparing Project for Web Deployment\nTo prepare your project for web deployment via GitHub Pages, we have three quick steps that we must first complete.\nFirst, in the “_quarto.yml” file, add output-dir: docs as a subheading beneath the project: heading. Make sure that the indentation is the same as the type: website but the new line can be either above or below that line.\n\n\n\nSecond, in the “Terminal” pane run touch .nojekyll. This creates a file called “.nojekyll” that is necessary for hosting your website via GitHub Pages.\nThird, in the “Terminal” pane run quarto render. This processes the template .qmd files you currently have in the repository and prepares them to become actual web pages.\nOnce you’ve done these three things you can move on to creating a GitHub repository so that we can take the necessary steps to having GitHub host your website!\n\n\nMake a New GitHub Repository\nFrom your GitHub “Repositories” tab, click the  green  “New” button.\n\nAdd a title to your repository and add a description. Once you’ve added these two things, scroll down and click the  green  “Create repository” button.\n\n\n\nBe sure that you do not add a README, do not add a gitignore, and do not add a license. Adding any of these three will cause a merge conflict when we link the project that you just created with the GitHub repository that you are in the process of creating.\n\n\n\nAfter a few seconds you should be placed on your new repository’s landing page which will look like the below image because there isn’t anything in your repository (yet).\nCopy the link in the field and go back to your RStudio session.\n\n\n\n\n\nAdding your Project to GitHub\nThe following steps include a sequence of command line operations that will be relayed in code chunks below. Unless otherwise stated, all of the following code should be run in “Terminal”.\nIf you didn’t check the “Create a git repository” button while creating the R project, you’ll need to do that via the command line now. If you did check that box, you should skip this step!\n\n# Start a git repository on the \"main\" branch\ngit init -b main\n\nStage all of the files in your project to the git repository. This includes the .yml file, all .qmd files and all of their rendered versions created when you ran quarto render earlier. This code is equivalent to checking the box for the files in the “Git” pane of RStudio.\n\n# Stage all files\ngit add .\n\nOnce everything has been staged, you now must commit those staged files with a message.\n\n# Commit all files with the message in quotes\ngit commit -m \"Initial commit\"\n\nNow that your project files have been committed, you need to tell your computer where you will be pushing to and pulling from. Paste the link you copied at the end of the “Make a New GitHub Repository” into the code shown in the chunk below (inside of the [...]) and run it.\n\n# Tell your computer which GitHub repository to connect to\ngit remote add origin [GITHUB_URL]\n\nVerify that URL before continuing.\n\n# Confirm that URL worked\ngit remote -v\n\nFinally, push your commited changes to the repostory that you set as the remote in the preceding two steps.\n\n# Push all of the content to the main branch\ngit push -u origin main\n\nNow, go back to GitHub and refresh the page to see your project content safe and sound in your new GitHub repository!\n\n\n\nDeploy Website via GitHub\nIn order to get your new website actually on the web, we’ll need to tell GitHub that we want our website to be accessible at a .github.io URL.\nTo do this, go to the “Settings” tab with a gear icon and click it. You may be prompted to re-enter your GitHub password, do so and you can proceed.\n\nIn the resulting page, look towards the bottom of the left sidebar of settings categories and click the “Pages” option. This is at the very bottom of the sidebar in the screen capture below but is towards the middle of all of the settings categories Github offers you.\n\nScroll down to the middle of this page and where it says “Branch” click the dropdown menu that says “None” by default.\n\nSelect “main” from the dropdown.\n\n\n\nThis opens up a new dropdown menu where you can select which folder in your repository contains your website’s content (it defaults to “/ (root)”). Because we specified output-dir: docs in the .yml file earlier we can select “/docs” from the dropdown menu.\n\n\n\nOnce you’ve told GitHub that you want a website generated from the “docs” folder on the main branch, click the “Save” button.\n\n\n\nFrom this moment your website has begun being deployed by GitHub! You can check the status of the building process by navigating to the “Actions” tab of your repository.\nSelect the “pages build and deployment workflow” in the list of workflows on the bottom righthand side of the page.\n\nThis shows you GitHub’s building and deployment process as a flowchart. While it is working on each step there will be an amber circle next to the name of that sub-task. When a sub-task is completed, the amber circle becomes a green circle with a check mark.\n\nWhen the three steps are complete the amber clock symbol next to the “pages build and deployment” action will turn into a larger green circle with a check mark. This is GitHub’s way of telling you that your website is live and accessible to anyone on the internet.\n\nYou can now visit your website by visiting its dedicated URL. This URL can be found by returning to the “Settings” tab and then scrolling through the sidebar to the “Pages” section.\nAlternately, the website for your repository always uses the following composition: https://repository owner.github.io/repository name/\n\nIf we visit that link, we can see that our website is live!\n\n\nGitHub Housekeeping\nWe recommend a quick housekeeping step now to make it easier to find this URL in the future. Copy the URL from the Pages setting area and return to the “Code” tab of the repository.\nOnce there, click the small gear icon to the right of the “About” header.\n\nIn the resulting window, paste the copied URL into the “Website” field. Once you’ve pasted it in, click the green “Save changes” button.\n\n\n\nThis places the link to your deployed website in an intuitive, easy-to-find location both for interested third parties and yourself in the future.\n\n\n\n\nAdding Website Content\nNow that you have a live website you can build whatever you’d like! Given the wide range of possibility, we’ll only cover how to add a new page but the same process applies to any edit to the living webpage.\nTo add a new page create a new Quarto document. You can do this by going to the “File” menu, entering the “New File” options, and selecting “Quarto Document…”\n\n\n\nSimilarly to an R markdown file, this will open a new window that lets you enter a title and author as well as decide what format you want to render files to along with some other settings options. You only need to click the “Create” button in the bottom right of this dialogue (though you can definitely play with the other options and text boxes as you desire).\n\n\n\nAfter a moment, a new .qmd file will open in Quarto’s visual editor. For the purposes of this tutorial, you only need to add a title in the top of the file but for a real website you can add whatever content sparks joy for you!\n\n\n\nSave that file into your project folder. Its name can be anything but be sure that you remember what you name it!\n\n\n\nAdd the name of the new Quarto document to the .yml file in the website navbar area (in this example the file is called “more-stuff.qmd”).\n\n\n\nOnce you’ve added the file to the fundamental architecture of your website, you need to tell Quarto to re-build the part of the website that GitHub looks for when it deploys. To do this run quarto render in the Terminal.\nIf you want to preview your changes, run quarto preview in the Terminal and a new browser window will be displayed showing your current website content. This preview continues until you click the red stop sign icon in RStudio so be sure to end it when you’re done with the preview!\n\nRegardless, once you’ve run either quarto render or quarto preview you need to stage and commit all changed files indicated in the Git pane of RStudio. As a reminder, to stage files you check the box next to them, to commit staged files, type an informative message and press the “Commit” button in the right side of the window.\n\nSwitch back to GitHub and you’ll see an amber dot next to the commit hash just beneath and to the left of the green “Code” button.\n\nWhen the amber dot turns into a green check mark that means that your edits to your website are now included in the live version of your site!\n\nWhen you visit your website you may need to refresh the page for your edits to appear but all new visitors will see the updated content when they load the page.\n\n\n\nSupplementary Information\nQuarto is developing at a rapid pace so quality of life changes and new functionalities are introduced relatively frequently. Additionally, Quarto supports user-created “extensions” that can be downloaded in a given project and then used (similar to the way user-developed R packages can be shared) so if you want to do something that Quarto itself doesn’t support, chances are you’ll be able to find an extension that handles it.\nQuarto’s documentation of website creation and formatting is extremely thorough and is a great resource as you become more comfortable with your new website. We hope this tutorial was useful to you and welcome constructively critical feedback! Please post an issue with any thoughts for improvement that you have."
  },
  {
    "objectID": "modules_tutorials/github-create.html",
    "href": "modules_tutorials/github-create.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "From your GitHub “Repositories” tab, click the  green  “New” button.\n\nAdd a title to your repository and add a description. Once you’ve added these two things, scroll down and click the  green  “Create repository” button.\n\n\n\nBe sure that you do not add a README, do not add a gitignore, and do not add a license. Adding any of these three will cause a merge conflict when we link the project that you just created with the GitHub repository that you are in the process of creating.\n\n\n\nAfter a few seconds you should be placed on your new repository’s landing page which will look like the below image because there isn’t anything in your repository (yet).\nCopy the link in the field and go back to your RStudio session."
  },
  {
    "objectID": "modules_tutorials/googledrive-auth.html",
    "href": "modules_tutorials/googledrive-auth.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "In order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone.\n\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!"
  },
  {
    "objectID": "modules_tutorials/quarto-website_new-proj.html",
    "href": "modules_tutorials/quarto-website_new-proj.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "To begin, click the “Project” button in the top right of your RStudio session.\n\nIn the resulting dialogue, click the “New Directory” option.\n\n\n\nFrom the list of options for project templates, select “Quarto Website”.\n\n\n\nPick a title and check the “Create a git repository” checkbox. For your title, short but descriptive titles are most effective. Once that is done, click “Create Project” in the bottom right of the window.\n\n\n\nAfter a few seconds, RStudio should refresh with a Quarto document (such documents have the file extension “.qmd”) and a “_quarto.yml” file open.\n\n\n\nPart of Quarto’s central philosophy is that all of the formatting of individual .qmd files in a project is governed by the settings created by a singular .yml file. In an R markdown project some of the global settings are set in .yml but other settings are handled within each .Rmd file. This centralization is a key innovation in streamlining projects and is one reason for Quarto’s quick popularity."
  },
  {
    "objectID": "modules_tutorials/quarto-website_add-content.html",
    "href": "modules_tutorials/quarto-website_add-content.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "Now that you have a live website you can build whatever you’d like! Given the wide range of possibility, we’ll only cover how to add a new page but the same process applies to any edit to the living webpage.\nTo add a new page create a new Quarto document. You can do this by going to the “File” menu, entering the “New File” options, and selecting “Quarto Document…”\n\n\n\nSimilarly to an R markdown file, this will open a new window that lets you enter a title and author as well as decide what format you want to render files to along with some other settings options. You only need to click the “Create” button in the bottom right of this dialogue (though you can definitely play with the other options and text boxes as you desire).\n\n\n\nAfter a moment, a new .qmd file will open in Quarto’s visual editor. For the purposes of this tutorial, you only need to add a title in the top of the file but for a real website you can add whatever content sparks joy for you!\n\n\n\nSave that file into your project folder. Its name can be anything but be sure that you remember what you name it!\n\n\n\nAdd the name of the new Quarto document to the .yml file in the website navbar area (in this example the file is called “more-stuff.qmd”).\n\n\n\nOnce you’ve added the file to the fundamental architecture of your website, you need to tell Quarto to re-build the part of the website that GitHub looks for when it deploys. To do this run quarto render in the Terminal.\nIf you want to preview your changes, run quarto preview in the Terminal and a new browser window will be displayed showing your current website content. This preview continues until you click the red stop sign icon in RStudio so be sure to end it when you’re done with the preview!\n\nRegardless, once you’ve run either quarto render or quarto preview you need to stage and commit all changed files indicated in the Git pane of RStudio. As a reminder, to stage files you check the box next to them, to commit staged files, type an informative message and press the “Commit” button in the right side of the window.\n\nSwitch back to GitHub and you’ll see an amber dot next to the commit hash just beneath and to the left of the green “Code” button.\n\nWhen the amber dot turns into a green check mark that means that your edits to your website are now included in the live version of your site!\n\nWhen you visit your website you may need to refresh the page for your edits to appear but all new visitors will see the updated content when they load the page."
  },
  {
    "objectID": "modules_tutorials/quarto-website_deploy-prep.html",
    "href": "modules_tutorials/quarto-website_deploy-prep.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "To prepare your project for web deployment via GitHub Pages, we have three quick steps that we must first complete.\nFirst, in the “_quarto.yml” file, add output-dir: docs as a subheading beneath the project: heading. Make sure that the indentation is the same as the type: website but the new line can be either above or below that line.\n\n\n\nSecond, in the “Terminal” pane run touch .nojekyll. This creates a file called “.nojekyll” that is necessary for hosting your website via GitHub Pages.\nThird, in the “Terminal” pane run quarto render. This processes the template .qmd files you currently have in the repository and prepares them to become actual web pages.\nOnce you’ve done these three things you can move on to creating a GitHub repository so that we can take the necessary steps to having GitHub host your website!"
  },
  {
    "objectID": "modules_tutorials/github-connect.html",
    "href": "modules_tutorials/github-connect.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "The following steps include a sequence of command line operations that will be relayed in code chunks below. Unless otherwise stated, all of the following code should be run in “Terminal”.\nIf you didn’t check the “Create a git repository” button while creating the R project, you’ll need to do that via the command line now. If you did check that box, you should skip this step!\n\n# Start a git repository on the \"main\" branch\ngit init -b main\n\nStage all of the files in your project to the git repository. This includes the .yml file, all .qmd files and all of their rendered versions created when you ran quarto render earlier. This code is equivalent to checking the box for the files in the “Git” pane of RStudio.\n\n# Stage all files\ngit add .\n\nOnce everything has been staged, you now must commit those staged files with a message.\n\n# Commit all files with the message in quotes\ngit commit -m \"Initial commit\"\n\nNow that your project files have been committed, you need to tell your computer where you will be pushing to and pulling from. Paste the link you copied at the end of the “Make a New GitHub Repository” into the code shown in the chunk below (inside of the [...]) and run it.\n\n# Tell your computer which GitHub repository to connect to\ngit remote add origin [GITHUB_URL]\n\nVerify that URL before continuing.\n\n# Confirm that URL worked\ngit remote -v\n\nFinally, push your commited changes to the repostory that you set as the remote in the preceding two steps.\n\n# Push all of the content to the main branch\ngit push -u origin main\n\nNow, go back to GitHub and refresh the page to see your project content safe and sound in your new GitHub repository!"
  },
  {
    "objectID": "modules_tutorials/github-website-deploy.html",
    "href": "modules_tutorials/github-website-deploy.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "In order to get your new website actually on the web, we’ll need to tell GitHub that we want our website to be accessible at a .github.io URL.\nTo do this, go to the “Settings” tab with a gear icon and click it. You may be prompted to re-enter your GitHub password, do so and you can proceed.\n\nIn the resulting page, look towards the bottom of the left sidebar of settings categories and click the “Pages” option. This is at the very bottom of the sidebar in the screen capture below but is towards the middle of all of the settings categories Github offers you.\n\nScroll down to the middle of this page and where it says “Branch” click the dropdown menu that says “None” by default.\n\nSelect “main” from the dropdown.\n\n\n\nThis opens up a new dropdown menu where you can select which folder in your repository contains your website’s content (it defaults to “/ (root)”). Because we specified output-dir: docs in the .yml file earlier we can select “/docs” from the dropdown menu.\n\n\n\nOnce you’ve told GitHub that you want a website generated from the “docs” folder on the main branch, click the “Save” button.\n\n\n\nFrom this moment your website has begun being deployed by GitHub! You can check the status of the building process by navigating to the “Actions” tab of your repository.\nSelect the “pages build and deployment workflow” in the list of workflows on the bottom righthand side of the page.\n\nThis shows you GitHub’s building and deployment process as a flowchart. While it is working on each step there will be an amber circle next to the name of that sub-task. When a sub-task is completed, the amber circle becomes a green circle with a check mark.\n\nWhen the three steps are complete the amber clock symbol next to the “pages build and deployment” action will turn into a larger green circle with a check mark. This is GitHub’s way of telling you that your website is live and accessible to anyone on the internet.\n\nYou can now visit your website by visiting its dedicated URL. This URL can be found by returning to the “Settings” tab and then scrolling through the sidebar to the “Pages” section.\nAlternately, the website for your repository always uses the following composition: https://repository owner.github.io/repository name/\n\nIf we visit that link, we can see that our website is live!\n\n\nGitHub Housekeeping\nWe recommend a quick housekeeping step now to make it easier to find this URL in the future. Copy the URL from the Pages setting area and return to the “Code” tab of the repository.\nOnce there, click the small gear icon to the right of the “About” header.\n\nIn the resulting window, paste the copied URL into the “Website” field. Once you’ve pasted it in, click the green “Save changes” button.\n\n\n\nThis places the link to your deployed website in an intuitive, easy-to-find location both for interested third parties and yourself in the future."
  },
  {
    "objectID": "onboard-scaffold_suggested-tools.html",
    "href": "onboard-scaffold_suggested-tools.html",
    "title": "Suggested Tools",
    "section": "",
    "text": "We primarily work on collaborative projects where we synthesize existing data to draw larger inferences than any single data set would allow. Because of this, we strongly recommend that each tool used by a team accomplish as many purposes as possible to avoid a project accruing endless “one off” tools that fit a specific purpose but do not accomplish any other tasks. Streamlining your workflow to just a few broadly useful programs also helps reduce the barrier to entry to training new team members and ensures that within team protocols are clear and concise to follow.\nThe analytical software options available at NCEAS follow directly from this ethos. Although occasionally providing specialty programs (upon request), we have otherwise carefully assembled a powerful lineup of scripted, cross-platform, scalable applications that are well-supported, generate robust results, and permit batch processing. Although these packages require an initial time investment to learn, and may seem intimidating to scientists familiar with only “point-and-click” software, we strongly argue that the long-term payoff is well worth the time investment at the start.\nWe recommend the following programs, websites, and platforms:"
  },
  {
    "objectID": "onboard-scaffold_suggested-tools.html#communication-tools",
    "href": "onboard-scaffold_suggested-tools.html#communication-tools",
    "title": "Suggested Tools",
    "section": "Communication Tools",
    "text": "Communication Tools\n\nVideo Conferencing\n\nBetween your in-person meetings at NCEAS (and during them if you have remote participants!) we recommend that you use Zoom. During the COVID-19 pandemic, Zoom became–arguably–the video chatting program and the value of your group’s familiarity with this program should not be underestimated.\n\n\nMessaging\n\nYour group will be doing a lot of written communication so we recommend creating a Slack community. The advantage of Slack over email is that you can have faster real-time conversations among multiple people without crowding your inbox. Slack also supports attaching files and “pinning” posts to make it easier to find them later. You can also create “channels” for sub-topics within your group (e.g., “paper 1”, “analysis”, etc.) that will allow your group members to track only the subtopics that are relevant to them. You can download the Slack desktop app here. The NCEAS Computing team has created this guide for getting set up on Slack.\n\n\nData Sharing\n\nWe recommend that you share data among your group using Google Drive. This is in part because many people are already at least somewhat familiar with Google Drive. In addition, there is an R package called googledrive that will allow you to directly connect any R scripts to Google Drive files and folders. This can be great for ensuring that all scripts use the same raw data and can be useful for exporting synthesized data products to a given Drive folder.\nNCEAS also has several analytical servers that can be a great place to store data that are too large for Google Drive to store easily. We’ll discuss NCEAS’ servers in greater detail further on in the onboarding process."
  },
  {
    "objectID": "onboard-scaffold_suggested-tools.html#computing-tools",
    "href": "onboard-scaffold_suggested-tools.html#computing-tools",
    "title": "Suggested Tools",
    "section": "Computing Tools",
    "text": "Computing Tools\n\nAnalysis\n\nOur team is most well-versed in R and many data synthesis ends can be accomplished in this language. R’s primary advantage is its amazing user community! R users have developed all kinds of custom functions and packages so even when what you want to do is not supported by an R package on CRAN, chances are that you can find the tools you need online!\n\nWe strongly recommend using R through RStudio as the RStudio interface allows several ‘quality of life’ improvements that you will grow to greatly appreciate if you’re not already an RStudio user. RStudio enables an easier connection to git (see below), facilitates use of the command line, and allows you to generate PDF or HTML reports with embedded code chunks for easy sharing in and outside of your group.\n\nIf your group is working with genomes or other large data files, you may find R’s memory limit to be a serious hindrance. If this is the case (or if you prefer to not use R!) we suggest Python as an alternative. Python works better with larger files and is also a well-respected programming language.\n\n\nCode Storage & Versioning\n\ngit is a great program to use for tracking changes to your code as your project grows and evolves. git is one type of “version control” software which is specifically built to track changes to code in an informative and useful way. It is analogous to using “track changes” in Microsoft Word but is built in a more seamless way. You can install git following the instructions here.\n\nSimilarly to R versus RStudio, we recommend that you create an account with GitHub so that the changes you track with git can be viewed and interacted with in a straightforward way. GitHub is the industry standard for tracking changes to code and is a great way of making code for a specific project publicly-accessible once you are at a stage where that feels appropriate. We offer an introductory workshop on git and GitHub that we are happy to offer to your group if that is of interest!"
  },
  {
    "objectID": "modules_wg-onboarding/services-trainings.html",
    "href": "modules_wg-onboarding/services-trainings.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "It is sometimes the case that your working group wants to become more familiar (or get a refresher) on a tool you’d like to include in your group’s workflow. To that end we can design and offer workshops on a selection of data science tools. Previous workshops include an introduction to GitHub, a primer on the tidyverse R packages, and a review of the fundamentals of R Shiny apps. Our past workshops are summarized here and we are happy to offer new workshops if your group wants a workshop on something within our knowledge base. These workshops are typically 1-2 hours but we can be flexible with that timing depending on your group’s needs.\nSimilarly, we also have been creating more ‘go at your own pace’-style tutorials that can be accessed here. These tutorials are usually smaller in scope than workshops but are still built to maximize value to your group either as review or first contact with a given subject. As with the workshops, we are happy to create new tutorials if something comes up for your team so please reach out and we can discuss further!"
  },
  {
    "objectID": "modules_wg-onboarding/services-office-hours.html",
    "href": "modules_wg-onboarding/services-office-hours.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "Each of our staff members offers a one-hour block weekly as a standing office hour each week. This is a great time to join us with small hurdles or obstacles you’re experiencing in a given week. For example, previous office hours have dealt with topics like refreshing on git/GitHub vocabulary, authenticating the googledrive R package, or solving a specific error in a new R script."
  },
  {
    "objectID": "modules_wg-onboarding/infrastructure-server.html",
    "href": "modules_wg-onboarding/infrastructure-server.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "Working groups have access to several high-performance computers at NCEAS, which provide advanced analytical, web, and database capabilities that far exceed the capabilities of desktop computers. We are available to set up and instruct you in the use of these Linux/Unix systems for demanding scientific analyses and modeling runs that benefit from lots of memory or storage, or access to multiple CPU’s.\n\nOur most powerful system (pictured) currently offers 384GB of RAM memory, along with 44 cores (CPUs), and several terabytes of fast storage. Several scientific software packages – such as R/RStudio, Python, Matlab, and QGIS – are already installed. Ask us if you need us to install any specific analytical libraries or packages.\nWe can also set up shared storage space on our server to facilitate data sharing within your working group. This is particularly valuable for large files that would take up a significant amount of your group’s cloud storage system (e.g., GoogleDrive, Dropbox, etc.)\nTo request access to the NCEAS analytical server, please coordinate with your PIs and contact us with your request."
  },
  {
    "objectID": "modules_wg-onboarding/services-sprints.html",
    "href": "modules_wg-onboarding/services-sprints.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "For larger tasks, we offer “analytical sprints.” If your group requests an analytical sprint, you will get one of our team members working full time only for your group’s tasks for 3-4 weeks! This can be a great option if your group has many smaller tasks or fewer larger projects that are particularly time-sensitive.\nWe are excited that these sprints are a part of our “menu” of offerings to you all but please reach out to us to start the conversation before requesting a sprint so that we can make sure we are all on the same page."
  },
  {
    "objectID": "modules_wg-onboarding/infrastructure-data-archive.html",
    "href": "modules_wg-onboarding/infrastructure-data-archive.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "While we know that you are just getting started, it is never too early to start thinking about where you are going to store the data your group will collate and synthesize! NCEAS is committed to practicing and promoting open science, making scientific research and its supporting data and information accessible to all levels of society. Therefore, we recommend that any input data used for your synthesis work be well documented and preserved in a long-term data repository.\n\nIt is required to document and preserve any products resulting from working group activities in a long-term data repository, such as the Environmental Data Initiative (EDI) data repository or the NCEAS-supported KNB Data Repository, which are part of the Data Observation Network for Earth (DataONE).\n\nWe are happy to discuss which of these options might fit your group best though we recognize this discussion can probably be left alone until your group has amassed a substantial amount of data."
  },
  {
    "objectID": "modules_wg-onboarding/services-tasks.html",
    "href": "modules_wg-onboarding/services-tasks.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "This level of collaboration is the core of our value to working groups! When your group identifies a data-related need (e.g., designing an analytical workflow, creating a website, writing an R Shiny app, etc.), you reach out to our team and get the conversation started. During that time we will work closely with you to define the scope of the work and get a clear picture of what “success” looks like in this context.\nOnce the task is appropriately defined, the conversation moves on to how independently you’d like us to work. This varies dramatically between tasks even within a single working group and there is no single right answer! For some tasks, we are capable of working completely independently and returning to your team with a finished product in hand for review but we are equally comfortable working closely with you throughout the life-cycle of a task."
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "In addition to the specific task-based support we offer, we can also create and run interactive workshops on data or coding topics. The specific goals of these workshops can be modified to best suit your team and meet all attendees where they are to ensure no one is left behind. While we are always happy to discuss developing new workshops we do have some materials that have already been designed (and tested by other working groups!) and are happy to offer any of these workshops to your group if it is of interest."
  },
  {
    "objectID": "workshops.html#collaborative-coding-with-github",
    "href": "workshops.html#collaborative-coding-with-github",
    "title": "Workshops",
    "section": "Collaborative Coding with GitHub",
    "text": "Collaborative Coding with GitHub\n\nWorkshop Website\n\nIn synthesis science, collaboration on code products is often integral to the productivity of the group. However, learning to use the software and graphical user interfaces that support this kind of teamwork can be a significant hurdle for teams that are already experts in their subject areas. This workshop is aimed at helping participants gain an understanding of the fundamental purpose and functioning of “version control” systems–specifically GitHub–to help teams more effectively code collaboratively.\nThe GitHub  repository for the workshop can be found here."
  },
  {
    "objectID": "workshops.html#coding-in-the-tidyverse",
    "href": "workshops.html#coding-in-the-tidyverse",
    "title": "Workshops",
    "section": "Coding in the tidyverse",
    "text": "Coding in the tidyverse\n\nWorkshop Website\n\nFor teams that code using the R programming language, the most familiar tools are often part of “base R” meaning that those functions and packages come pre-loaded when R is installed. Relatively recently the tidyverse has emerged as a comprehensive suite of packages that can complement base R or serve as an alternative for some tasks. This includes packages like dplyr and tidyr as well as the perhaps infamous pipe operator (%>%) among many other tools. This workshop is aimed at helping participants use the tidyverse equivalents of fundamental data wrangling tasks that learners may be used to performing with base R.\nThe GitHub  repository for the workshop can be found here."
  },
  {
    "objectID": "workshops.html#r-shiny-apps-for-sharing-science",
    "href": "workshops.html#r-shiny-apps-for-sharing-science",
    "title": "Workshops",
    "section": "R Shiny Apps for Sharing Science",
    "text": "R Shiny Apps for Sharing Science\n\nWorkshop Website\n\nOne of our team members–Nick Lyon–created a workshop on learning to create R Shiny apps. R Shiny includes a suite of R packages (primarily shiny) that allow R users to create interactive apps that can be subsequently deployed to a URL. These apps are most commonly used for data visualization purposes but can also be a neat way of accomplishing other outward-facing tasks without needing to learn a new programming language. This workshop was offered at the 2022 LTER All Scientists’ Meeting (ASM) and is aimed at an audience with moderate R capability but limited prior exposure to Shiny.\nThe GitHub  repository for the workshop can be found here.\nThis workshop also includes a second GitHub  repository that contains several example Shiny apps. See here."
  },
  {
    "objectID": "workshops.html#other-training-resources",
    "href": "workshops.html#other-training-resources",
    "title": "Workshops",
    "section": "Other Training Resources",
    "text": "Other Training Resources\n\nNCEAS Learning Hub\n\n\nTraining Catalog\n\nIn addition to the workshops described above, NCEAS offers a variety of other workshops and trainings that may be of interest to you or your group via the Learning Hub. While these trainings can be very helpful, it is important to note that our team may or may not be involved with teaching them. Also, workshops we create will be hosted on this website rather than on the Learning Hub.\n\n\nThe Carpentries\n\nThe Carpentries is another great place to find workshops and tutorials on various data and programming topics. All of their materials are publicly available so even if a workshop isn’t being offered, you can visit that site and review the content at your own pace! This can be a nice way of refreshing yourself on the fundamentals of something you have prior experience with or teaching yourself something totally new! For example, the Carpentries include helpful workshops on using R for ecologists, using the “shell” or command line, or handling geospatial data in R.\nFor the set of lessons that are most likely to be helpful to your groups, explore the Data Carpentry and Software Carpentry lesson lists."
  },
  {
    "objectID": "sci-comp-definition.html",
    "href": "sci-comp-definition.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "In the most fundamental sense, scientific computing refers to the process of organizing, managing, and analyzing scientific data using computers. This process takes place at the interface between several distinct disciplines:\n\nInformatics provides both a planning framework and operational rules for acquiring, handling, interpreting, and storing the underlying information in a useful and efficient manner\nComputer science and the technologies it produces provide the overarching computational environment, including the core processing “engine” and a means to control it\nRelevant branches of mathematics, statistics, and probability arm us with numerical models, algorithms, error representations, and other constructs for describing and solving problems quantitatively\nLast but not least, the target science itself (e.g., ecology) provides an overall motivation, research paradigm, and conceptual model to guide the analysis\n\nWith these many links to other fields, modern scientific computing is now viewed as a field of study unto itself, constantly evolving in step with advances in the disciplines on which it is based.\n\n\n\nRegal Fritillary on Milkweed, Konza Prairie Biological Station - Photographer: Jill Haukos"
  },
  {
    "objectID": "tutorial-scaffold_googledrive-auth.html",
    "href": "tutorial-scaffold_googledrive-auth.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "The googledrive R package is a package that lets R users directly interact with files on GoogleDrive. This can be extremely useful because it lets all members of a team share the same source data file(s) and guarantees that updates to “living” documents are received by all group members the next time they run their R script. This package is technically part of the Tidyverse but is not loaded by running library(tidyverse).\nBecause this package requires access to an R user’s GoogleDrive, you must “authenticate” the googledrive package. This essentially tells Google that it is okay if an R package uses your credentials to access and (potentially) modify your Drive content. There are only a few steps to this process but follow along with the below tutorial and we’ll get you ready to integrate the Google Drive into your code workflows using the googledrive package in no time!\n\nPrerequisites\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nCreate a Gmail account\n\nFeel free to skip any steps that you have already completed!\n\n\nAuthorize googledrive\nIn order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone.\n\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!\n\n\n\n\n\nSupplementary Information\nIf you have tried to use drive_auth and did not check the box indicated above, you need to make the googledrive package ask you again. Using drive_auth will not (annoyingly) return you to the place it sent you the first time. However, if you run the following code chunk it should give you another chance to check the needed box.\nThe gargle R package referenced below is required for interacting with Google Application Program Interfaces (APIs). This package does the heavy lifting of secure password and token management and is necessary for the googledrive authentication chunk below.\n\ngoogledrive::drive_auth(\n  email = gargle::gargle_oauth_email(),\n  path = NULL,\n  scopes = \"https://www.googleapis.com/auth/drive\",\n  cache = gargle::gargle_oauth_cache(),\n  use_oob = gargle::gargle_oob_default(),\n  token = NULL)\n\nUnfortunately, to use the googledrive package you must check the box that empowers the package to function as designed. If you’re uncomfortable giving the googledrive that much power you will need to pivot your workflow away from using GoogleDrive directly. However, NCEAS does offer access to an internal server called “Aurora” where data can be securely saved and shared among group members without special authentication like what googledrive requires. Reach out to our team if this seems like a more attractive option for your working group and we can offer training on how to use this powerful tool!"
  },
  {
    "objectID": "best_practices.html",
    "href": "best_practices.html",
    "title": "Coding Tips",
    "section": "",
    "text": "This page contains the collected best practice tips of the NCEAS Scientific Computing Support Team. More will be added over time and feel free to post an issue if you have a specific request for a section to add to this document.\nPlease feel free to reach out to our team (see here) if you have any questions about this best practices manual and/or need help implementing some of this content.\nCheck the headings below or in the table of contents on the right of this page to see which tips and tricks we have included so far and we hope this page is a useful resource to you and your team!"
  },
  {
    "objectID": "best_practices.html#r-scripts-versus-r-markdowns",
    "href": "best_practices.html#r-scripts-versus-r-markdowns",
    "title": "Coding Tips",
    "section": "R Scripts versus R Markdowns",
    "text": "R Scripts versus R Markdowns\nWhen coding in R, either R scripts (.R files) or R markdowns (.Rmd files) are viable options but they have different advantages and disadvantages that we will cover below.\n\nR Scripts - Positives\nR scripts’ greatest strength is their flexibility. They allow you to format a file in whatever way is most intuitive to you. Additionally, R scripts can be cleaner for for loops insofar as they need not be concerned with staying within a given code chunk (as would be the case for a .Rmd). Developing a new workflow can be swiftly accomplished in an R script as some or all of the code in a script can be run by simply selecting the desired lines rather than manually running the desired chunks in a .Rmd file. Finally, R scripts can also be a better home for custom functions that can be sourced by another file (even a .Rmd!) for making repeated operations simpler to read.\n\n\nR Scripts - Potential Weaknesses\nThe benefit of extreme flexibility in R scripts can sometimes be a disadvantage however. We’ve all seen (and written) R scripts that have few or no comments or where lines of code are densely packed without spacing or blank lines to help someone new to the code understand what is being done. R scripts can certainly be written in a way that is accessible to those without prior knowledge of what the script accomplishes but they do not enforce such structure. This can make it easy, especially when we’re feeling pressed for time, to exclude structure that helps our code remain reproducible and understandable.\n\n\nR Markdowns - Positives\nR markdown files’ ability to “knit” as HTML or PDF documents makes them extremely useful in creating outward-facing reports. This is particularly the case when the specific code is less important to communicate than visualizations and/or analyses of the data but .Rmd files do facilitate echoing the code so that report readers can see how background operations were accomplished. The code chunk structure of these files can also nudge users towards including valuable comments (both between chunks and within them) though of course .Rmd files do not enforce such non-code content.\n\n\nR Markdowns - Potential Weaknesses\nR markdowns can fail to knit due to issues even when the code within the chunks works as desired. Duplicate code chunk names or a failure to install LaTeX can be a frustrating hurdle to overcome between functioning code and a knit output file. When code must be re-run repeatedly (as is often the case when developing a new workflow) the stop-and-start nature of running each code chunk separately can also be a small irritation.\n\n\nScript vs. Markdown Summary\nTaken together, both R scripts and R markdown files can empower users to write reproducible, transparent code. However, both file types have some key limitations that should be taken into consideration when choosing which to use as you set out to create a new code product."
  },
  {
    "objectID": "best_practices.html#file-paths",
    "href": "best_practices.html#file-paths",
    "title": "Coding Tips",
    "section": "File Paths",
    "text": "File Paths\nThis section contains our recommendations for handling file paths. When you code collaboratively (e.g., with GitHub), accounting for the difference between your folder structure and those of your colleagues becomes critical. Ideally your code should be completely agnostic about (1) the operating system of the computer it is running on (i.e., Windows vs. Mac) and (2) the folder structure of the computer. We can–fortunately–handle these two considerations relatively simply.\nThis may seem somewhat dry but it is worth mentioning that failing to use relative file paths is a significant hindrance to reproducibility (see Trisovic et al. 2022).\n\n1. Preserve File Paths as Objects Using file.path\nDepending on the operating system of the computer, the slashes between folder names are different (\\ versus /). The file.path function automatically detects the computer operating system and inserts the correct slash. We recommend using this function and assigning your file path to an object.\n\nmy_path <- file.path(\"path\", \"to\", \"my\", \"file\")\nmy_path\n\n[1] \"path/to/my/file\"\n\n\nOnce you have that path object, you can use it everywhere you import or export information to/from the code (with another use of file.path to get the right type of slash!).\n\n# Import\nmy_raw_data <- read.csv(file = file.path(my_path, \"raw_data.csv\"))\n\n# Export\nwrite.csv(x = data_object, file = file.path(my_path, \"tidy_data.csv\"))\n\n\n\n2. Create Necessary Sub-Folders in the Code with dir.create\nUsing file.path guarantees that your code will work regardless of the upstream folder structure but what about the folders that you need to export or import things to/from? For example, say your graphs.R script saves a couple of useful exploratory graphs to the “Plots” folder, how would you guarantee that everyone running graphs.R has a “Plots folder”? You can use the dir.create function to create the folder in the code (and include your path object from step 1!).\n\n# Create needed folder\ndir.create(path = file.path(my_path, \"Plots\"), showWarnings = FALSE)\n\n# Then export to that folder\nggplot2::ggsave(filename = file.path(my_path, \"Plots\", \"my_plot.png\"))\n\nThe showWarnings argument of dir.create simply warns you if the folder you’re creating already exists or not. There is no negative to “creating” a folder that already exists (nothing is overwritten!!) but the warning can be confusing so we can silence it ahead of time.\n\n\nFile Paths Summary\nWe strongly recommend following these guidelines so that your scripts work regardless of (1) the operating system, (2) folders “upstream” of the working directory, and (3) folders within the project. This will help your code by flexible and reproducible when others are attempting to re-run your scripts!"
  },
  {
    "objectID": "best_practices.html#package-loading",
    "href": "best_practices.html#package-loading",
    "title": "Coding Tips",
    "section": "Package Loading",
    "text": "Package Loading\nLoading packages / libraries in R can be cumbersome when working collaboratively because there is no guarantee that you all have the same packages installed. While you could comment-out an install.packages() line for every package you need for a given script, we recommend using the R package librarian to greatly simplify this process!\nlibrarian::shelf() accepts the names of all of the packages–either CRAN or GitHub–installs those that are missing in that particular R session and then attaches all of them. See below for an example:\nTo load packages typically you’d have something like the following in your script:\n\n## Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"devtools\")\n# devtools::install_github(\"NCEAS/scicomptools\")\n\n# Load libraries\nlibrary(tidyverse); library(scicomptools)\n\nWith librarian::shelf() however this becomes much cleaner! In addition to being fewer lines, using librarian also removes the possibility that someone running your code misses one of the packages that your script depends on and then the script breaks for them later on. librarian::shelf() automatically detects whether a package is installed, installs it if necessary, and then attaches the package.\nIn essence, librarian::shelf() wraps install.packages(), devtools::install_github(), and library() into a single, human-readable function.\n\n# Install and load packages!\nlibrarian::shelf(tidyverse, NCEAS/scicomptools)\n\nWhen using librarian::shelf(), package names do not need to be quoted and GitHub packages can be installed without the additional steps of installing the devtools package and using devtools::install_github() instead of install.packages()."
  },
  {
    "objectID": "index.html#what-we-do",
    "href": "index.html#what-we-do",
    "title": "Scientific Computing Support",
    "section": "What We Do",
    "text": "What We Do\nThe Scientific Computing team is a small (but mighty!) team of data analysts/scientists supporting Long Term Ecological Research (LTER) Network synthesis working groups. The LTER Network Office (LNO) regularly issues calls for synthesis working group proposals, which are open to scientists from within and outside of the LTER Network. We provide modern technological infrastructure to support analytical, computing, or network-based needs for these synthesis working groups.\nWe are housed at the National Center for Ecological Analysis and Synthesis (NCEAS). In addition to the technical support during your visit at NCEAS, our scientific computing team is available in-between visits to discuss and advise on data science and scientific programming tasks, such as:\n\nStructuring and integrating heterogeneous datasets\nWriting code to wrangle, analyze, model, or visualize the data your group has already collected\nDesigning workflows, scripting best practices for reproducible science, and reviewing code\nHelping you get set up on NCEAS’ server\nPreserving and promoting your products on the Web - from derived datasets and terminological glossaries/vocabularies, to scripts, model codes, and interactive “web applications”\nOffering workshops on new skills or programs\n\nDepending on your team’s preferences, we can operate on a spectrum of independence ranging from complete self-sufficiency after initial definition of task scope to coding together with your team.\nContact our team with your requests at scicomp@nceas.ucsb.edu"
  },
  {
    "objectID": "index.html#navigating-this-website",
    "href": "index.html#navigating-this-website",
    "title": "Scientific Computing Support",
    "section": "Navigating this Website",
    "text": "Navigating this Website\n\nIf you’re a working group member…\nFor members of our working groups, this website is a centralized hub of resources designed to help you get familiar with the way our team works and what we offer. The pages under the Working Group Onboarding tab contain materials that will facilitate collaboration between our team and your group!\nVisit Working Together for more details on the types of tasks we can help with.\nFor information on the modern technological infrastructure that NCEAS provides, check out NCEAS Infrastructure.\nTo see a list of the supported, collaborative tools that our team uses, visit Suggested Tools.\nFor other miscellaneous resources, see Other Resources.\nWe look forward to assisting your group however we can! Please do not hesitate to contact our team for any task requests and questions.\n\n\nFor working groups and all our other visitors…\nWelcome! Even if you’re not a part of a synthesis working group, we hope that you find the various resources on this website helpful! If you’re interested in learning more about open, reproducible science, please feel free to take a look at our Workshops on GitHub, tidyverse, and R Shiny apps! Or peruse our Tutorials and Coding Tips pages for other useful tips in R.\n\n\n\nLTER All Scientists’ Meeting 2022, Monterey CA"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Some of the content that we produce is not as detailed as our full workshops but has a wider scope than the content included in our Coding Tips suggestions. This information can broadly be defined as “tutorials” though their depth and scope can vary significantly depending on the topic being tutorialized. As with our Coding Tips, this page is under constant development so please post an issue if you have an idea for a tutorial that you’d like to suggest that we create."
  },
  {
    "objectID": "tutorials.html#using-the-googledrive-r-package",
    "href": "tutorials.html#using-the-googledrive-r-package",
    "title": "Tutorials",
    "section": "Using the googledrive R Package",
    "text": "Using the googledrive R Package\nThe googledrive R package is a package that lets R users directly interact with files on GoogleDrive. This can be extremely useful because it lets all members of a team share the same source data file(s) and guarantees that updates to “living” documents are received by all group members the next time they run their R script. This package is technically part of the Tidyverse but is not loaded by running library(tidyverse).\nBecause this package requires access to an R user’s GoogleDrive, you must “authenticate” the googledrive package. This essentially tells Google that it is okay if an R package uses your credentials to access and (potentially) modify your Drive content. There are only a few steps to this process but follow along with the below tutorial and we’ll get you ready to integrate the Google Drive into your code workflows using the googledrive package in no time!\n\nPrerequisites\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nCreate a Gmail account\n\nFeel free to skip any steps that you have already completed!\n\n\nAuthorize googledrive\nIn order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone.\n\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!\n\n\n\n\n\nSupplementary Information\nIf you have tried to use drive_auth and did not check the box indicated above, you need to make the googledrive package ask you again. Using drive_auth will not (annoyingly) return you to the place it sent you the first time. However, if you run the following code chunk it should give you another chance to check the needed box.\nThe gargle R package referenced below is required for interacting with Google Application Program Interfaces (APIs). This package does the heavy lifting of secure password and token management and is necessary for the googledrive authentication chunk below.\n\ngoogledrive::drive_auth(\n  email = gargle::gargle_oauth_email(),\n  path = NULL,\n  scopes = \"https://www.googleapis.com/auth/drive\",\n  cache = gargle::gargle_oauth_cache(),\n  use_oob = gargle::gargle_oob_default(),\n  token = NULL)\n\nUnfortunately, to use the googledrive package you must check the box that empowers the package to function as designed. If you’re uncomfortable giving the googledrive that much power you will need to pivot your workflow away from using GoogleDrive directly. However, NCEAS does offer access to an internal server called “Aurora” where data can be securely saved and shared among group members without special authentication like what googledrive requires. Reach out to our team if this seems like a more attractive option for your working group and we can offer training on how to use this powerful tool!"
  },
  {
    "objectID": "tutorials.html#building-a-website-with-quarto",
    "href": "tutorials.html#building-a-website-with-quarto",
    "title": "Tutorials",
    "section": "Building a Website with Quarto",
    "text": "Building a Website with Quarto\nQuarto is a new tool developed by RStudio (the company, not the program) to create a more ‘what you see is what you get’ editor for creating markdown files and products (e.g., books, websites, etc.). Additionally, it includes a visual editor that allows users to insert headings and embed figures via buttons that are intuitively labeled rather than through somewhat arcane HTML text or symbols. While Quarto is still in its infancy, it is rapidly gathering a following due to the aforementioned visual editor and for the ease with which quarto documents and websites can be created.\n\nPrerequisites\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nDownload Quarto\nInstall git\nCreate a GitHub Account\nConnect git to GitHub\n\nFeel free to skip any steps that you have already completed!\n\n\nCreate a Quarto Website R Project\nTo begin, click the “Project” button in the top right of your RStudio session.\n\nIn the resulting dialogue, click the “New Directory” option.\n\n\n\nFrom the list of options for project templates, select “Quarto Website”.\n\n\n\nPick a title and check the “Create a git repository” checkbox. For your title, short but descriptive titles are most effective. Once that is done, click “Create Project” in the bottom right of the window.\n\n\n\nAfter a few seconds, RStudio should refresh with a Quarto document (such documents have the file extension “.qmd”) and a “_quarto.yml” file open.\n\n\n\nPart of Quarto’s central philosophy is that all of the formatting of individual .qmd files in a project is governed by the settings created by a singular .yml file. In an R markdown project some of the global settings are set in .yml but other settings are handled within each .Rmd file. This centralization is a key innovation in streamlining projects and is one reason for Quarto’s quick popularity.\n\n\nPreparing Project for Web Deployment\nTo prepare your project for web deployment via GitHub Pages, we have three quick steps that we must first complete.\nFirst, in the “_quarto.yml” file, add output-dir: docs as a subheading beneath the project: heading. Make sure that the indentation is the same as the type: website but the new line can be either above or below that line.\n\n\n\nSecond, in the “Terminal” pane run touch .nojekyll. This creates a file called “.nojekyll” that is necessary for hosting your website via GitHub Pages.\nThird, in the “Terminal” pane run quarto render. This processes the template .qmd files you currently have in the repository and prepares them to become actual web pages.\nOnce you’ve done these three things you can move on to creating a GitHub repository so that we can take the necessary steps to having GitHub host your website!\n\n\nMake a New GitHub Repository\nFrom your GitHub “Repositories” tab, click the  green  “New” button.\n\nAdd a title to your repository and add a description. Once you’ve added these two things, scroll down and click the  green  “Create repository” button.\n\n\n\nBe sure that you do not add a README, do not add a gitignore, and do not add a license. Adding any of these three will cause a merge conflict when we link the project that you just created with the GitHub repository that you are in the process of creating.\n\n\n\nAfter a few seconds you should be placed on your new repository’s landing page which will look like the below image because there isn’t anything in your repository (yet).\nCopy the link in the field and go back to your RStudio session.\n\n\n\n\n\nAdding your Project to GitHub\nThe following steps include a sequence of command line operations that will be relayed in code chunks below. Unless otherwise stated, all of the following code should be run in “Terminal”.\nIf you didn’t check the “Create a git repository” button while creating the R project, you’ll need to do that via the command line now. If you did check that box, you should skip this step!\n\n# Start a git repository on the \"main\" branch\ngit init -b main\n\nStage all of the files in your project to the git repository. This includes the .yml file, all .qmd files and all of their rendered versions created when you ran quarto render earlier. This code is equivalent to checking the box for the files in the “Git” pane of RStudio.\n\n# Stage all files\ngit add .\n\nOnce everything has been staged, you now must commit those staged files with a message.\n\n# Commit all files with the message in quotes\ngit commit -m \"Initial commit\"\n\nNow that your project files have been committed, you need to tell your computer where you will be pushing to and pulling from. Paste the link you copied at the end of the “Make a New GitHub Repository” into the code shown in the chunk below (inside of the [...]) and run it.\n\n# Tell your computer which GitHub repository to connect to\ngit remote add origin [GITHUB_URL]\n\nVerify that URL before continuing.\n\n# Confirm that URL worked\ngit remote -v\n\nFinally, push your commited changes to the repostory that you set as the remote in the preceding two steps.\n\n# Push all of the content to the main branch\ngit push -u origin main\n\nNow, go back to GitHub and refresh the page to see your project content safe and sound in your new GitHub repository!\n\n\n\nDeploy Website via GitHub\nIn order to get your new website actually on the web, we’ll need to tell GitHub that we want our website to be accessible at a .github.io URL.\nTo do this, go to the “Settings” tab with a gear icon and click it. You may be prompted to re-enter your GitHub password, do so and you can proceed.\n\nIn the resulting page, look towards the bottom of the left sidebar of settings categories and click the “Pages” option. This is at the very bottom of the sidebar in the screen capture below but is towards the middle of all of the settings categories Github offers you.\n\nScroll down to the middle of this page and where it says “Branch” click the dropdown menu that says “None” by default.\n\nSelect “main” from the dropdown.\n\n\n\nThis opens up a new dropdown menu where you can select which folder in your repository contains your website’s content (it defaults to “/ (root)”). Because we specified output-dir: docs in the .yml file earlier we can select “/docs” from the dropdown menu.\n\n\n\nOnce you’ve told GitHub that you want a website generated from the “docs” folder on the main branch, click the “Save” button.\n\n\n\nFrom this moment your website has begun being deployed by GitHub! You can check the status of the building process by navigating to the “Actions” tab of your repository.\nSelect the “pages build and deployment workflow” in the list of workflows on the bottom righthand side of the page.\n\nThis shows you GitHub’s building and deployment process as a flowchart. While it is working on each step there will be an amber circle next to the name of that sub-task. When a sub-task is completed, the amber circle becomes a green circle with a check mark.\n\nWhen the three steps are complete the amber clock symbol next to the “pages build and deployment” action will turn into a larger green circle with a check mark. This is GitHub’s way of telling you that your website is live and accessible to anyone on the internet.\n\nYou can now visit your website by visiting its dedicated URL. This URL can be found by returning to the “Settings” tab and then scrolling through the sidebar to the “Pages” section.\nAlternately, the website for your repository always uses the following composition: https://repository owner.github.io/repository name/\n\nIf we visit that link, we can see that our website is live!\n\n\nGitHub Housekeeping\nWe recommend a quick housekeeping step now to make it easier to find this URL in the future. Copy the URL from the Pages setting area and return to the “Code” tab of the repository.\nOnce there, click the small gear icon to the right of the “About” header.\n\nIn the resulting window, paste the copied URL into the “Website” field. Once you’ve pasted it in, click the green “Save changes” button.\n\n\n\nThis places the link to your deployed website in an intuitive, easy-to-find location both for interested third parties and yourself in the future.\n\n\n\n\nAdding Website Content\nNow that you have a live website you can build whatever you’d like! Given the wide range of possibility, we’ll only cover how to add a new page but the same process applies to any edit to the living webpage.\nTo add a new page create a new Quarto document. You can do this by going to the “File” menu, entering the “New File” options, and selecting “Quarto Document…”\n\n\n\nSimilarly to an R markdown file, this will open a new window that lets you enter a title and author as well as decide what format you want to render files to along with some other settings options. You only need to click the “Create” button in the bottom right of this dialogue (though you can definitely play with the other options and text boxes as you desire).\n\n\n\nAfter a moment, a new .qmd file will open in Quarto’s visual editor. For the purposes of this tutorial, you only need to add a title in the top of the file but for a real website you can add whatever content sparks joy for you!\n\n\n\nSave that file into your project folder. Its name can be anything but be sure that you remember what you name it!\n\n\n\nAdd the name of the new Quarto document to the .yml file in the website navbar area (in this example the file is called “more-stuff.qmd”).\n\n\n\nOnce you’ve added the file to the fundamental architecture of your website, you need to tell Quarto to re-build the part of the website that GitHub looks for when it deploys. To do this run quarto render in the Terminal.\nIf you want to preview your changes, run quarto preview in the Terminal and a new browser window will be displayed showing your current website content. This preview continues until you click the red stop sign icon in RStudio so be sure to end it when you’re done with the preview!\n\nRegardless, once you’ve run either quarto render or quarto preview you need to stage and commit all changed files indicated in the Git pane of RStudio. As a reminder, to stage files you check the box next to them, to commit staged files, type an informative message and press the “Commit” button in the right side of the window.\n\nSwitch back to GitHub and you’ll see an amber dot next to the commit hash just beneath and to the left of the green “Code” button.\n\nWhen the amber dot turns into a green check mark that means that your edits to your website are now included in the live version of your site!\n\nWhen you visit your website you may need to refresh the page for your edits to appear but all new visitors will see the updated content when they load the page.\n\n\n\nSupplementary Information\nQuarto is developing at a rapid pace so quality of life changes and new functionalities are introduced relatively frequently. Additionally, Quarto supports user-created “extensions” that can be downloaded in a given project and then used (similar to the way user-developed R packages can be shared) so if you want to do something that Quarto itself doesn’t support, chances are you’ll be able to find an extension that handles it.\nQuarto’s documentation of website creation and formatting is extremely thorough and is a great resource as you become more comfortable with your new website. We hope this tutorial was useful to you and welcome constructively critical feedback! Please post an issue with any thoughts for improvement that you have."
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Our Team",
    "section": "",
    "text": "Because we live in an era where we may only meet in person sporadically, we felt it would be nice to introduce ourselves here to help you put a face to the emails / Slack messages / GitHub issues we exchange going forward! If you would like to email the whole team as one send your questions to scicomp@nceas.ucsb.edu"
  },
  {
    "objectID": "staff.html#julien-brun",
    "href": "staff.html#julien-brun",
    "title": "Our Team",
    "section": "Julien Brun",
    "text": "Julien Brun\nbrunj7.github.io –  brunj7 –  @brunj7 – brun@nceas.ucsb.edu\n\nAs a senior data scientist, the core of Julien’s work is to understand the data and computing challenges researchers are facing and help them to translate these challenges into solvable tasks. Julien advises and mentors on how to clean, structure, combine, and analyze their heterogeneous data sets, as well as scaling up their analysis while promoting open and reproducible data science principles.\nJulien is also a Lecturer in the Master in Environmental Data Science program at Bren School of Environmental Science and Management at UC Santa Barbara, where he teaches “good enough” practices in reproducible and collaborative data science."
  },
  {
    "objectID": "staff.html#angel-chen",
    "href": "staff.html#angel-chen",
    "title": "Our Team",
    "section": "Angel Chen",
    "text": "Angel Chen\n angelchen7 – anchen@nceas.ucsb.edu\n\nAngel supports working groups by developing data pipelines and reproducible analytical workflows to integrate various sources of data. Angel previously worked as a data curator for the Arctic Data Center, helping researchers archive and store their data. Angel recently completed a B.S. in statistics & data science at the University of California, Santa Barbara."
  },
  {
    "objectID": "staff.html#nick-lyon",
    "href": "staff.html#nick-lyon",
    "title": "Our Team",
    "section": "Nick Lyon",
    "text": "Nick Lyon\nnjlyon0.github.io –  njlyon0 –  @scilyon – lyon@nceas.ucsb.edu\n\nNick focuses on supporting working groups in the data acquisition and management prerequisite to analysis and visualization. Nick is a trained restoration ecologist who specialized on interacting communities of plants and insects and has extensive experience taking “raw” field-collected data and readying it for hypothesis testing in a rigorous, transparent way. Nick completed his M.S. in Ecology and Evolutionary Biology at Iowa State University"
  },
  {
    "objectID": "team-onboarding.html",
    "href": "team-onboarding.html",
    "title": "Sci Comp Team Onboarding",
    "section": "",
    "text": "This page contains a quick, cookbook-style set of instructions for new staff on the Scientific Computing Support Team. We are so excited to have you join us and we hope that these instructions are clear and easy-to-follow. Please don’t hesitate to reach out if you run into any issues (see the “Our Team” tab of this site) or post a GitHub issue yourself on this website’s repository!\nListed below are a mix of references and tutorials on concepts we aim to promote to researchers who we support, as well as tools and workflows we use in our team. The goal is to give context about open and reproducible science principles NCEAS is promoting to the scientific community. You will also find information on getting access to the tools that you will need and some general information about working at NCEAS."
  },
  {
    "objectID": "team-onboarding.html#background-reading-material",
    "href": "team-onboarding.html#background-reading-material",
    "title": "Sci Comp Team Onboarding",
    "section": "Background Reading Material",
    "text": "Background Reading Material\n\nHampton et al 2015. “The Tao of open science for ecology”\nBorer et al 2009. “Effective Data Management”\nFegraus et al 2005. “Maximizing the Value of Ecological Data with Structured Metadata:”\nHeidborn 2008. “Shedding Light on the Dark Data in the Long Tail of Science”"
  },
  {
    "objectID": "team-onboarding.html#programs-resources",
    "href": "team-onboarding.html#programs-resources",
    "title": "Sci Comp Team Onboarding",
    "section": "Programs & Resources",
    "text": "Programs & Resources\n\nGitHub Tutorial\nWe rely heavily on GitHub for collaboration, and there are a few things you must do to get set up. First, complete the NCEAS GitHub Tutorial here. This tutorial will help you with getting started with git and GitHub using RStudio.\n\n\nLTER GitHub Organization\nRegister (if you have not already) for a personal GitHub account and send your username to Julien to be added to the LTER GitHub. This GitHub Organization “owns” the working groups repositories that you will be directly working with so being added to this organization will give you access to needed repositories.\n\n\nNCEAS GitHub Enterprise Organization\nWe use a GitHub Enterprise account to keep track of tasks for the Scientific Computing Support of LTER working groups. To be added to the account you must create an NCEAS account.\nFollow the directions outlined here and send your username to Julien Brun (not the email outlined in the Google Doc).\nAfter you have been added, you will be able to access the Issues tab of the lter-wg-scicomp GitHub repository. We use issues on this repository to keep track of tasks and projects as well as who is primarily responsible for a given task and which working group gave us the task initially.\n\n\nSlack\nWe use Slack to communicate with one another throughout the day. To be added to the NCEAS Slack group, register here. If you already have a slack account, be sure to use the email address you used to register.\nFind and join our #scicomp channel by clicking the plus sign next to the Channels section. Feel free to join any other channels that you might find interesting! Popular channels include #diversity, #nceas, and #social.\nFinally, complete this short tutorial on using Slack.\n\n\nNCEAS Server: Aurora\nWe use the Aurora server (located at aurora.nceas.ucsb.edu) when working with RStudio or JupyterHub. Send an email to help@nceas.ucsb.edu requesting an account mentioning you are working with Julien. Thomas Hetmank or Nick Outin will contact you with directions for setting up an account."
  },
  {
    "objectID": "team-onboarding.html#recurring-nceas-meetings",
    "href": "team-onboarding.html#recurring-nceas-meetings",
    "title": "Sci Comp Team Onboarding",
    "section": "Recurring NCEAS Meetings",
    "text": "Recurring NCEAS Meetings\nThere are a number of recurring meetings that you are encouraged to attend. At your hiring, you should recieve an invitation to the NCEAS Google Calendar which has links to all the meeting information. These include Coffee Klatch (a coffee social), Hacky Hours, and Data Science Chats (to name a few).\nWe also have a team Google Calendar to manage team events. Julien will add you if you have not been already."
  },
  {
    "objectID": "team-onboarding.html#data-science-background-and-tutorials",
    "href": "team-onboarding.html#data-science-background-and-tutorials",
    "title": "Sci Comp Team Onboarding",
    "section": "Data Science Background and Tutorials",
    "text": "Data Science Background and Tutorials\nFor good background information on the tools that we use, read through and practice examples of the following chapters from a training we did for postdocs in early 2020: https://science-for-nature-and-people.github.io/2020-data-collab-workshop/2020-02-snapp/index.html\nChapters 3, 5, 8 are a good background about the tools we use.\nChapters 9, 10 are a good introduction to data modeling.\n\nR Self-Assessment\nFinally, please complete this R Training Assessment to self-assess your skills in R.\nSchedule a session with Julien to debrief on your experience."
  },
  {
    "objectID": "team-onboarding.html#hr-related-resources",
    "href": "team-onboarding.html#hr-related-resources",
    "title": "Sci Comp Team Onboarding",
    "section": "HR-related Resources",
    "text": "HR-related Resources\n\nReporting Hours & Kronos\nWe submit vacation and sick day hours online through Kronos. You should receive an email at the start of your employment adding you to an email listserv that will send reminders on when to approve each month’s timesheet.\n\n\nUC Path\nUC Path is the online HR portal and can be accessed with your UCSB Net ID. It is where you can access things like your paycheck, next date of paycheck, and benefits (if applicable)."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "While much of the work we do is specific to a given working group or task, sometimes we realize afterwards that our functions have the potential to be useful beyond the scope for which they were initially written. To that end, we have created the R package scicomptools!\n\n\n\nTo install the package in R, use the following:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"NCEAS/scicomptools\")\n\n\n\n\nThis package contains a diverse mix of functions for everything from repetitive data wrangling tasks to checking whether you have a token attached for GitHub. In addition, functions that we wrote that are deprecated (usually because their internal workings have been superseded by packages on CRAN) are removed from the package but retained in the GitHub repository in case they are useful to you! All functions–both live and deprecated–are summarized in the README on the GitHub repository so take a look!"
  },
  {
    "objectID": "portfolio.html#r-shiny-apps",
    "href": "portfolio.html#r-shiny-apps",
    "title": "Portfolio",
    "section": "R Shiny Apps",
    "text": "R Shiny Apps\nOur team created another R package–lterpalettefinder–to extract color palettes from user-supplied photos. To help non-R users still explore this package and have fun pulling color palettes from photos, we created this standalone Shiny app. This app lets anyone interact with lterpalettefinder via their browser with no R experience required!\nWe have also built apps to help working groups visualize data or present survey results in an interactive, visually-appealing format. Shiny apps can also include useful ‘overview’ portions that serve as an excellent landing page for third parties to your group’s activities!"
  },
  {
    "objectID": "portfolio.html#project-websites",
    "href": "portfolio.html#project-websites",
    "title": "Portfolio",
    "section": "Project Websites",
    "text": "Project Websites\nOur team also helped build a website for the Soil Organic Matter (SOM) working group. One of this group’s primary products was a synthesized data package containing observed data, modifications of that data, and models based on them. The website operates in part to publicize this data product but also to provide a central location for other resources developed by or important to the SOM group.\nFor your group we can (if desired):\n\nBuild a website using Quarto\n\nAll website content creation can be done via RStudio which your group may already be somewhat familiar with\nQuarto also offers a new “visual editor” that lets you format text as you would in any word processor (i.e., Microsoft Word, Pages, etc.)\n\nMaintain the website OR help you to maintain it\n\nQuarto is written entirely in “Markdown syntax” which makes it easily maintained by either our team or yours depending on your preference\nWe have also created a tutorial on making websites with Quarto that you are welcome to explore!"
  },
  {
    "objectID": "portfolio.html#spatial-analyses-wrangling",
    "href": "portfolio.html#spatial-analyses-wrangling",
    "title": "Portfolio",
    "section": "Spatial Analyses & Wrangling",
    "text": "Spatial Analyses & Wrangling\nOur team has also been instrumental in acquiring, wrangling, and summarizing spatial data. For the Silica Exports working group we developed a workflow (see here) that accomplishes the following tasks:\n\nCreates shapefiles that identify the drainage basin linked to several hundred stream gages\n“Cookie cuts” spatial data within those drainage basins (e.g., land cover, lithology, precipitation, etc.)\nSummarizes that extracted data both as rasters and as data tables for use as explanatory variables in other analyses\n\nWe are comfortable working with such data and can help your team acquire and/or process spatial data if that is of interest!"
  },
  {
    "objectID": "modules_best-practices/markdown-vs-script.html",
    "href": "modules_best-practices/markdown-vs-script.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "When coding in R, either R scripts (.R files) or R markdowns (.Rmd files) are viable options but they have different advantages and disadvantages that we will cover below.\n\nR Scripts - Positives\nR scripts’ greatest strength is their flexibility. They allow you to format a file in whatever way is most intuitive to you. Additionally, R scripts can be cleaner for for loops insofar as they need not be concerned with staying within a given code chunk (as would be the case for a .Rmd). Developing a new workflow can be swiftly accomplished in an R script as some or all of the code in a script can be run by simply selecting the desired lines rather than manually running the desired chunks in a .Rmd file. Finally, R scripts can also be a better home for custom functions that can be sourced by another file (even a .Rmd!) for making repeated operations simpler to read.\n\n\nR Scripts - Potential Weaknesses\nThe benefit of extreme flexibility in R scripts can sometimes be a disadvantage however. We’ve all seen (and written) R scripts that have few or no comments or where lines of code are densely packed without spacing or blank lines to help someone new to the code understand what is being done. R scripts can certainly be written in a way that is accessible to those without prior knowledge of what the script accomplishes but they do not enforce such structure. This can make it easy, especially when we’re feeling pressed for time, to exclude structure that helps our code remain reproducible and understandable.\n\n\nR Markdowns - Positives\nR markdown files’ ability to “knit” as HTML or PDF documents makes them extremely useful in creating outward-facing reports. This is particularly the case when the specific code is less important to communicate than visualizations and/or analyses of the data but .Rmd files do facilitate echoing the code so that report readers can see how background operations were accomplished. The code chunk structure of these files can also nudge users towards including valuable comments (both between chunks and within them) though of course .Rmd files do not enforce such non-code content.\n\n\nR Markdowns - Potential Weaknesses\nR markdowns can fail to knit due to issues even when the code within the chunks works as desired. Duplicate code chunk names or a failure to install LaTeX can be a frustrating hurdle to overcome between functioning code and a knit output file. When code must be re-run repeatedly (as is often the case when developing a new workflow) the stop-and-start nature of running each code chunk separately can also be a small irritation.\n\n\nScript vs. Markdown Summary\nTaken together, both R scripts and R markdown files can empower users to write reproducible, transparent code. However, both file types have some key limitations that should be taken into consideration when choosing which to use as you set out to create a new code product."
  },
  {
    "objectID": "modules_best-practices/pkg-loading.html",
    "href": "modules_best-practices/pkg-loading.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "Loading packages / libraries in R can be cumbersome when working collaboratively because there is no guarantee that you all have the same packages installed. While you could comment-out an install.packages() line for every package you need for a given script, we recommend using the R package librarian to greatly simplify this process!\nlibrarian::shelf() accepts the names of all of the packages–either CRAN or GitHub–installs those that are missing in that particular R session and then attaches all of them. See below for an example:\nTo load packages typically you’d have something like the following in your script:\n\n## Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"devtools\")\n# devtools::install_github(\"NCEAS/scicomptools\")\n\n# Load libraries\nlibrary(tidyverse); library(scicomptools)\n\nWith librarian::shelf() however this becomes much cleaner! In addition to being fewer lines, using librarian also removes the possibility that someone running your code misses one of the packages that your script depends on and then the script breaks for them later on. librarian::shelf() automatically detects whether a package is installed, installs it if necessary, and then attaches the package.\nIn essence, librarian::shelf() wraps install.packages(), devtools::install_github(), and library() into a single, human-readable function.\n\n# Install and load packages!\nlibrarian::shelf(tidyverse, NCEAS/scicomptools)\n\nWhen using librarian::shelf(), package names do not need to be quoted and GitHub packages can be installed without the additional steps of installing the devtools package and using devtools::install_github() instead of install.packages()."
  },
  {
    "objectID": "modules_best-practices/file-paths.html",
    "href": "modules_best-practices/file-paths.html",
    "title": "Scientific Computing Support",
    "section": "",
    "text": "This section contains our recommendations for handling file paths. When you code collaboratively (e.g., with GitHub), accounting for the difference between your folder structure and those of your colleagues becomes critical. Ideally your code should be completely agnostic about (1) the operating system of the computer it is running on (i.e., Windows vs. Mac) and (2) the folder structure of the computer. We can–fortunately–handle these two considerations relatively simply.\nThis may seem somewhat dry but it is worth mentioning that failing to use relative file paths is a significant hindrance to reproducibility (see Trisovic et al. 2022).\n\n1. Preserve File Paths as Objects Using file.path\nDepending on the operating system of the computer, the slashes between folder names are different (\\ versus /). The file.path function automatically detects the computer operating system and inserts the correct slash. We recommend using this function and assigning your file path to an object.\n\nmy_path <- file.path(\"path\", \"to\", \"my\", \"file\")\nmy_path\n\n[1] \"path/to/my/file\"\n\n\nOnce you have that path object, you can use it everywhere you import or export information to/from the code (with another use of file.path to get the right type of slash!).\n\n# Import\nmy_raw_data <- read.csv(file = file.path(my_path, \"raw_data.csv\"))\n\n# Export\nwrite.csv(x = data_object, file = file.path(my_path, \"tidy_data.csv\"))\n\n\n\n2. Create Necessary Sub-Folders in the Code with dir.create\nUsing file.path guarantees that your code will work regardless of the upstream folder structure but what about the folders that you need to export or import things to/from? For example, say your graphs.R script saves a couple of useful exploratory graphs to the “Plots” folder, how would you guarantee that everyone running graphs.R has a “Plots folder”? You can use the dir.create function to create the folder in the code (and include your path object from step 1!).\n\n# Create needed folder\ndir.create(path = file.path(my_path, \"Plots\"), showWarnings = FALSE)\n\n# Then export to that folder\nggplot2::ggsave(filename = file.path(my_path, \"Plots\", \"my_plot.png\"))\n\nThe showWarnings argument of dir.create simply warns you if the folder you’re creating already exists or not. There is no negative to “creating” a folder that already exists (nothing is overwritten!!) but the warning can be confusing so we can silence it ahead of time.\n\n\nFile Paths Summary\nWe strongly recommend following these guidelines so that your scripts work regardless of (1) the operating system, (2) folders “upstream” of the working directory, and (3) folders within the project. This will help your code by flexible and reproducible when others are attempting to re-run your scripts!"
  },
  {
    "objectID": "onboard-scaffold_other-materials.html#lter-network-office-guidance",
    "href": "onboard-scaffold_other-materials.html#lter-network-office-guidance",
    "title": "Other Resources",
    "section": "LTER Network Office Guidance",
    "text": "LTER Network Office Guidance\nThe LTER Network Office (LNO) has curated some useful resources on their website (lternet.edu) that we strongly recommend exploring! In particular, the LNO has centralized resources for working group primary investigators (PIs) (see here). Additionally, the LNO has created some broader resources for all members of working groups (see here).\nThe LTER Code of Conduct is also publicly available if you’d like to explore it."
  },
  {
    "objectID": "onboard-scaffold_other-materials.html#nceas-resources-for-working-groups",
    "href": "onboard-scaffold_other-materials.html#nceas-resources-for-working-groups",
    "title": "Other Resources",
    "section": "NCEAS’ Resources for Working Groups",
    "text": "NCEAS’ Resources for Working Groups\nNCEAS hosts many working groups and so has drawn from its wealth of experience to assemble a set of useful information on its Resources for Working Groups page.\nIn particular, we recommend checking out the documents nested in the “How to Run a Working Group” dropdown menu. Your group may also want to check out the “Reimbursement Forms” dropdown after your meetings in Santa Barbara to ensure that your eligible expenses get reimbursed!"
  },
  {
    "objectID": "onboard-scaffold_other-materials.html#diversity-equity-and-inclusion",
    "href": "onboard-scaffold_other-materials.html#diversity-equity-and-inclusion",
    "title": "Other Resources",
    "section": "Diversity, Equity, and Inclusion",
    "text": "Diversity, Equity, and Inclusion\nThe LTER Network and NCEAS are committed to improving diversity and inclusion in science. For the LTER Network Office’s diversity, equity and inclusion resources see here. For more information on NCEAS’ commitment to this mission–and related links–please visit NCEAS’ DEI page."
  },
  {
    "objectID": "onboard-scaffold_other-materials.html#visiting-santa-barbara",
    "href": "onboard-scaffold_other-materials.html#visiting-santa-barbara",
    "title": "Other Resources",
    "section": "Visiting Santa Barbara",
    "text": "Visiting Santa Barbara\nWhen your group is visiting at NCEAS’ office in beautiful Santa Barbara there are additional resources available to your group and opportunities for recreation and bonding as a team! Check out NCEAS’ general computing resources page for a quick rundown of some resources available only in Santa Barbara (including how to get on the WiFi!).\nWe very much recommend taking an afternoon or morning on one of the days of your visit to get out and explore Santa Barbara! NCEAS’ page for things to do in SB for new employees might prove to be a helpful resource for you and your group as you’re deciding where to eat and what to do!"
  }
]